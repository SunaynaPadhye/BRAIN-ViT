{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRAIN-ViT Model Inferencing for 3-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class labels: \n",
    "\n",
    "0: CN\n",
    "\n",
    "1: AD\n",
    "\n",
    "2: MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from data.data_loader_3c import NiftiDataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from config.BRAIN_ViT_config import get_config\n",
    "torch.manual_seed(100)\n",
    "\n",
    "config = get_config()\n",
    "device = config[\"device\"]\n",
    "\n",
    "\n",
    "#DEFINE YOUR BASE DIR PATH\n",
    "base_dir = \"----your path----\"\n",
    "\n",
    "# Load train, validation, and test datasets\n",
    "test_dataset = NiftiDataset(\"data/metadata.csv\", base_dir=base_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=True)\n",
    "print(\"test_loader:\",len(test_loader.dataset))\n",
    "\n",
    "model = torch.load('BRAIN-ViT.pt')\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "checkpoint_path = \"checkpoints/BRAIN-ViT_ckp.pth\"\n",
    "if checkpoint_path and os.path.isfile(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_accuracy = checkpoint.get('val_accuracy', 0.0)\n",
    "    print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# uncomment for infering on multiple GPUs\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get the predictions along with other metrics\n",
    "def evaluate_model(model, config, criterion, data_loader, action):    \n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    config['viz_attn_weights'] = False\n",
    "    config['viz_topk'] = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs, config)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Collect labels and predictions for metric calculations\n",
    "            all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            all_preds.extend(predicted.detach().cpu().numpy().tolist())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    total_loss = running_loss / len(data_loader)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score with macro averaging for multiclass classification\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    print(f\"{action.capitalize()} Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Loss:{total_loss:.4f}\")\n",
    "\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, preds = evaluate_model(model, config, criterion, test_loader, action=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:     \",list(labels))\n",
    "print(\"\\nPredictions:\",list(preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
